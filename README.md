# ğŸš— LightGBM Regression with Optuna Hyperparameter Tuning

This project implements a regression pipeline using [LightGBM](https://lightgbm.readthedocs.io/) with [Optuna](https://optuna.org/) for hyperparameter tuning. The primary goal is to predict continuous values (e.g., license plate prices) while optimizing the SMAPE (Symmetric Mean Absolute Percentage Error) metric.

---

## ğŸ”§ Features

- ğŸ“Š LightGBM for fast and accurate regression  
- ğŸ” Optuna for efficient hyperparameter optimization  
- ğŸ§ª Custom SMAPE metric for evaluation  
- ğŸ§¼ Data preprocessing and log-transform support  
- ğŸ“ˆ Train-validation split with early stopping

---

## ğŸ“ˆ Metric

We use **SMAPE** (Symmetric Mean Absolute Percentage Error), which is defined as:

$$
\text{SMAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \frac{|y_{\text{pred},i} - y_{\text{true},i}|}{(|y_{\text{true},i}| + |y_{\text{pred},i}|)/2}
$$

Implemented as a custom metric in LightGBM.

---

## ğŸ“¦ Dependencies

* lightgbm
* numpy
* pandas
* optuna
* scikit-learn

---

## ğŸ“œ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributions

Feel free to fork this repo and submit a pull request if you'd like to improve or extend it. Bug fixes, feature additions, and refactoring are welcome.

---

## ğŸ‘¤ Author

**Your Name**
[Linkedin](https://www.linkedin.com/in/dan4o/)
